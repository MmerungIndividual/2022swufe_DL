{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='Nadam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=10)\n",
    "_, acc=model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"acc:%.3f%\"%(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN识别手写数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 51s 84ms/step - loss: 0.1395 - accuracy: 0.9561\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 50s 83ms/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 50s 83ms/step - loss: 0.0295 - accuracy: 0.9908\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 50s 83ms/step - loss: 0.0233 - accuracy: 0.9925\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 51s 85ms/step - loss: 0.0165 - accuracy: 0.9948\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 51s 85ms/step - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 52s 87ms/step - loss: 0.0126 - accuracy: 0.9959\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 50s 83ms/step - loss: 0.0106 - accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 50s 84ms/step - loss: 0.0099 - accuracy: 0.9965\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 50s 84ms/step - loss: 0.0095 - accuracy: 0.9969\n",
      "313/313 - 3s - loss: 0.0258 - accuracy: 0.9924 - 3s/epoch - 10ms/step\n",
      "(None, 28, 28) - (None, 28, 28, 1)\n",
      "(None, 28, 28, 1) - (None, 28, 28, 32)\n",
      "(None, 28, 28, 32) - (None, 14, 14, 32)\n",
      "(None, 14, 14, 32) - (None, 14, 14, 64)\n",
      "(None, 14, 14, 64) - (None, 7, 7, 64)\n",
      "(None, 7, 7, 64) - (None, 3136)\n",
      "(None, 3136) - (None, 1024)\n",
      "(None, 1024) - (None, 1024)\n",
      "(None, 1024) - (None, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "n,m = x_train.shape[1], x_train.shape[2]\n",
    "model = Sequential()\n",
    "model.add(Reshape((n,m,1), input_shape=(n,m)))\n",
    "model.add(Conv2D(filters=32, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(filters=64, kernel_size=(5,5), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='Nadam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=100)\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(('%s - %s')%(layer.input_shape, layer.output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cpu'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "my_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,0.1307,0.1307), (0.3081,0.3081,0.3081)),\n",
    "    ])\n",
    "# 训练集\n",
    "train_ds = MNIST(\n",
    "    root='mnist',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=my_transform\n",
    ")\n",
    "# 测试集\n",
    "test_ds = MNIST(\n",
    "    root='mnist',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=my_transform\n",
    ")\n",
    "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 235 Loss:2.376 | Acc:10.547% (27/256)\n",
      "1 235 Loss:4.501 | Acc:11.523% (59/512)\n",
      "2 235 Loss:4.848 | Acc:9.505% (73/768)\n",
      "3 235 Loss:4.345 | Acc:9.668% (99/1024)\n",
      "4 235 Loss:4.046 | Acc:10.625% (136/1280)\n",
      "5 235 Loss:3.871 | Acc:11.133% (171/1536)\n",
      "6 235 Loss:3.731 | Acc:10.770% (193/1792)\n",
      "7 235 Loss:3.646 | Acc:10.986% (225/2048)\n",
      "8 235 Loss:3.598 | Acc:11.458% (264/2304)\n",
      "9 235 Loss:3.514 | Acc:11.836% (303/2560)\n",
      "10 235 Loss:3.424 | Acc:12.678% (357/2816)\n",
      "11 235 Loss:3.368 | Acc:13.086% (402/3072)\n",
      "12 235 Loss:3.309 | Acc:13.401% (446/3328)\n",
      "13 235 Loss:3.260 | Acc:13.393% (480/3584)\n",
      "14 235 Loss:3.197 | Acc:13.620% (523/3840)\n",
      "15 235 Loss:3.145 | Acc:13.599% (557/4096)\n",
      "16 235 Loss:3.091 | Acc:13.856% (603/4352)\n",
      "17 235 Loss:3.042 | Acc:14.497% (668/4608)\n",
      "18 235 Loss:2.996 | Acc:14.988% (729/4864)\n",
      "19 235 Loss:2.951 | Acc:15.566% (797/5120)\n",
      "20 235 Loss:2.904 | Acc:16.276% (875/5376)\n",
      "21 235 Loss:2.864 | Acc:16.744% (943/5632)\n",
      "22 235 Loss:2.817 | Acc:17.527% (1032/5888)\n",
      "23 235 Loss:2.775 | Acc:18.066% (1110/6144)\n",
      "24 235 Loss:2.737 | Acc:18.672% (1195/6400)\n",
      "25 235 Loss:2.698 | Acc:19.456% (1295/6656)\n",
      "26 235 Loss:2.656 | Acc:20.182% (1395/6912)\n",
      "27 235 Loss:2.613 | Acc:21.261% (1524/7168)\n",
      "28 235 Loss:2.575 | Acc:21.956% (1630/7424)\n",
      "29 235 Loss:2.538 | Acc:22.643% (1739/7680)\n",
      "30 235 Loss:2.503 | Acc:23.475% (1863/7936)\n",
      "31 235 Loss:2.460 | Acc:24.695% (2023/8192)\n",
      "32 235 Loss:2.420 | Acc:25.722% (2173/8448)\n",
      "33 235 Loss:2.379 | Acc:26.827% (2335/8704)\n",
      "34 235 Loss:2.343 | Acc:27.902% (2500/8960)\n",
      "35 235 Loss:2.309 | Acc:28.895% (2663/9216)\n",
      "36 235 Loss:2.273 | Acc:29.814% (2824/9472)\n",
      "37 235 Loss:2.239 | Acc:30.798% (2996/9728)\n",
      "38 235 Loss:2.203 | Acc:31.931% (3188/9984)\n",
      "39 235 Loss:2.167 | Acc:33.125% (3392/10240)\n",
      "40 235 Loss:2.132 | Acc:34.213% (3591/10496)\n",
      "41 235 Loss:2.099 | Acc:35.268% (3792/10752)\n",
      "42 235 Loss:2.068 | Acc:36.119% (3976/11008)\n",
      "43 235 Loss:2.035 | Acc:37.118% (4181/11264)\n",
      "44 235 Loss:2.004 | Acc:38.116% (4391/11520)\n",
      "45 235 Loss:1.977 | Acc:38.961% (4588/11776)\n",
      "46 235 Loss:1.949 | Acc:39.769% (4785/12032)\n",
      "47 235 Loss:1.920 | Acc:40.723% (5004/12288)\n",
      "48 235 Loss:1.891 | Acc:41.558% (5213/12544)\n",
      "49 235 Loss:1.864 | Acc:42.320% (5417/12800)\n",
      "50 235 Loss:1.839 | Acc:43.084% (5625/13056)\n",
      "51 235 Loss:1.814 | Acc:43.863% (5839/13312)\n",
      "52 235 Loss:1.788 | Acc:44.686% (6063/13568)\n",
      "53 235 Loss:1.762 | Acc:45.457% (6284/13824)\n",
      "54 235 Loss:1.738 | Acc:46.214% (6507/14080)\n",
      "55 235 Loss:1.715 | Acc:46.931% (6728/14336)\n",
      "56 235 Loss:1.692 | Acc:47.629% (6950/14592)\n",
      "57 235 Loss:1.671 | Acc:48.330% (7176/14848)\n",
      "58 235 Loss:1.649 | Acc:49.033% (7406/15104)\n",
      "59 235 Loss:1.630 | Acc:49.629% (7623/15360)\n",
      "60 235 Loss:1.611 | Acc:50.243% (7846/15616)\n",
      "61 235 Loss:1.589 | Acc:50.901% (8079/15872)\n",
      "62 235 Loss:1.569 | Acc:51.476% (8302/16128)\n",
      "63 235 Loss:1.552 | Acc:52.020% (8523/16384)\n",
      "64 235 Loss:1.533 | Acc:52.638% (8759/16640)\n",
      "65 235 Loss:1.514 | Acc:53.237% (8995/16896)\n",
      "66 235 Loss:1.496 | Acc:53.813% (9230/17152)\n",
      "67 235 Loss:1.478 | Acc:54.389% (9468/17408)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, 10)\n",
    "model.train()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for batch_idx,(inputs,targets) in enumerate(train_dl):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs,targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss+=loss.item()\n",
    "    _,predicted = outputs.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets).sum().item()\n",
    "    print(batch_idx,len(train_dl),'Loss:%.3f | Acc:%.3f%% (%d/%d)'\n",
    "         %(train_loss/(batch_idx+1),100.*correct/total,correct,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
